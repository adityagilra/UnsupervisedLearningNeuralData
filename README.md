# Unsupervised Fitting of Models to Neural Data  
  
Fitting various models to neural data in an unsupervised manner, i.e. without using the stimuli that generated the neural responses.  
This project is an ongoing collaboration with Gašper Tkačik and Michael J. Berry II.  
  
Currently, we try to fit two models:  
1. Model allowing temporal (independent or Hidden Markov Model) and spatial (independendent or tree) correlations, from the paper:  
Prentice, Jason S., Olivier Marre, Mark L. Ioffe, Adrianna R. Loback, Gašper Tkačik, and Michael J. Berry II. 2016. “Error-Robust Modes of the Retinal Population Code.” PLOS Computational Biology 12 (11): e1005148. [https://doi.org/10.1371/journal.pcbi.1005148](https://doi.org/10.1371/journal.pcbi.1005148).  
We use python bindings from a companion repository: [https://github.com/adityagilra/TreeHMM-local](https://github.com/adityagilra/TreeHMM-local) which has been forked (and slightly modified) from the original C++ code with Matlab bindings [https://github.com/adriannaloback/TreeHMM-local](https://github.com/adriannaloback/TreeHMM-local).  
  
2. Winner take all neural clustering algorithm:  
Loback, Adrianna R., and Michael J. Berry. 2018. “A Biologically Plausible Mechanism to Learn Clusters of Neural Activity.” BioRxiv, August, 389155. [https://doi.org/10.1101/389155](https://doi.org/10.1101/389155).
We use the python bindings from a companion repository: [https://github.com/adityagilra/VIWTA-SNN](https://github.com/adityagilra/VIWTA-SNN) which has been forked (and slightly modified) from the original C++ code with Matlab bindings [https://github.com/adriannaloback/VIWTA-SNN](https://github.com/adriannaloback/VIWTA-SNN).  
  
On three kinds of data:  
1. Experimental retinal data from Prentice et al 2016   
[https://datadryad.org/stash/dataset/doi:10.5061/dryad.1f1rc](https://datadryad.org/stash/dataset/doi:10.5061/dryad.1f1rc).  
  
2. Experimental retinal data from  
Tkačik, Gašper, Olivier Marre, Dario Amodei, Elad Schneidman, William Bialek, and Michael J. Berry II. 2014. “Searching for Collective Behavior in a Large Network of Sensory Neurons.” PLOS Computational Biology 10 (1): e1003408. [https://doi.org/10.1371/journal.pcbi.1003408](https://doi.org/10.1371/journal.pcbi.1003408).  
We use the neural responses to 297 repeats of a 19s long movie. The full data is available here  
[https://research-explorer.app.ist.ac.at/record/5562](https://research-explorer.app.ist.ac.at/record/5562)  
  
3. We also use data generated by an algorithm that varies the correlation between neurons parametrically between 0 to 1.9 (1 corresponds to experiment), while keeping individual neural firing rates constant in the dataset 2 above.  
The code for the same is part of this repo in the folder [data_generation](https://github.com/adityagilra/UnsupervisedLearningNeuralData/tree/master/data_generation). See below for details.  
  
-------------  
  
## Fitting with TreeHMM model by Prentice et al 2016:  
Clone the github repo: [https://github.com/adityagilra/TreeHMM-local](https://github.com/adityagilra/TreeHMM-local) into a folder called `TreeHMM` (shoudn't have `-local`). Run `make` in this folder after setting the right python and boost versions and directories. Libraries boost and boostpython must be present on your system. Set the PYTHONPATH to include its parent.  
  
The file `EMBasins_sbatch.py` can be called directly on the command line with an argument that decides the dataset, the number of latent modes, and the random number generator seed. The file can also be called in batch mode on a cluster using the SLURM system, i.e. `sbatch --array=0-659 submit_EMBasins.sbatch` where the taskid is passed in as the command line argument. The datasets 1, 2 and 3 above should be available in folders specified by the DataFileBase variable, which is set depending on the command line argument. You don't need to have all the datasets present. Taskids 0-599 are for generated dataset 3, 600 to 629 are for dataset 2, 630-659 are for dataset 1.  
  
You can choose between Hidden Markov Model vs time-independent model by calling pyHMM() or pyEMBasins() respectively (no need to recompile). See details in `EMBasins_sbatch.py`.  
  
Spatial correlations / tree term can be removed by modifying this statement at the top of EMBasins.cpp (need to recompile after this)  
 // Selects which basin model to use  
 typedef TreeBasin BasinType;  
 to  
 typedef IndependentBasin BasinType;  
Thus you can switch from HMM to EMBasins to remove time-domain correlations,  
 and TreeBasin to IndependentBasin to remove space-domain correlations.  
    
-------------  
  
## Fitting with Winner Take All neural model by Loback and Berry 2014:  
Clone the github repository: [https://github.com/adityagilra/VIWTA-SNN](https://github.com/adityagilra/VIWTA-SNN) into a folder called `VIWTA_SNN` (note: hyphen to underscore). Run `make` in this folder after setting the right python and boost versions and directories. Libraries boost and boostpython must be present on your system. Set the PYTHONPATH to include its parent.  
  
The file `WTAcluster_sbatch.py` and `submit_WTAcluster.sbatch` work in the same way as `EMBasins_sbatch.py` and `submit_EMBasins.sbatch` above. `WTAcluster_sbatch.py` can be called independently on the commandline with an argument, or via the SLURM system in batch mode on a cluster. `WTAcluster_sbatch.py` depends on `EMBasins_sbatch.py` for its pre-processing, so look at both to understand usage.  
  
-------------  
  
## Code (provided by Gašper Tkačik) in the data_generation folder for generating synthetic datasets parametrized by alpha which controls spatial correlation strength.  
  
Shared folder Synthsets has the parameters to generate various synthetic data.  
These params are pre-fitted for various alphas, ensuring that individual neural mean firing rates match the experimental data from Tkačik, et al 2014.  
  
synthset_k_X_Y_Z.mat means:  
X == number of neurons (always 4, meaning 120 neuron groups)  
Y == the replicate (the subset of 120 neurons from data being studied; there is a lot of overlap of neurons within the group)  
Z == integer representing alpha factor, alpha = synthset.factor, scales all correlations up and down.  
  
In each synthset, there are parameters of the model fit (K-pairwise), which are `[synthset.hs; synthset.js]` in matlab.  
  
### Python bindings:  
Run `make` in the data_generation directory after setting the right python and boost versions and directories. You'll need the boost and boostpython libraries installed. Then see `runMMCGen.py` on how to read in the synthsets and to generate sythetic data.  
  
### Matlab bindings:  
The sampling is called by Matlab to C source, which needs to be complied using matlab mex compiler, using  
`mex mxMaxentTGen.cpp mt19937ar.cpp`  
This will produce “mxMaxentTGen” which can be executed from matlab.  
  
This routine is called from runMMCGen.m script, which you would execute as follows:  
`[i1 i2 stats ee smp_mc] = runMMCGen([synthset.hs;synthset.js], 120, 100000, 100, round(rand()*10000000),'KSpikeIsing', 0);`  
samples would be in smp_mc matrix (binary)  
This is for 120 neurons, draw 100000 samples by recording a sample, doing 100 MC steps, recording a sample etc (so “100” is the sampling frequency). The round(rand()…) stuff is the initial random seed. ‘KSpikeIsing’ is the form of the model, 0 doesn’t matter.  
  
As a basic check of correctness, one should be able to plot the true mean firing rates vs the mean firing rates sampled from the model (since the pre-fitted synthsets pin the firing rate of each of 120 neurons to its observed value):  
`figure;plot(synthset.mv0, mean(smp_mc'),'ko’)`  
This should be close to a straight line.  
  
